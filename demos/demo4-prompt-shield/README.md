# Demo 4: Prompt Shield â€” Securing the Offer Analyzer

## Evolution from Demo 1

This demo is **not a new application** â€” it is the **same offer analysis agent from Demo 1**, with one critical addition: a **Prompt Shield pre-screening layer** that scans documents for indirect prompt injection before they reach the LLM.

| Component | Demo 1 (Vulnerable) | Demo 4 (Secured) |
|-----------|---------------------|-------------------|
| `agent.py` | âœ… Flock offer ranker | âœ… **Identical** â€” unchanged |
| `models.py` | âœ… Pydantic models | âœ… **Identical** â€” unchanged |
| `loader.py` | âœ… Markdown loader | âœ… **Identical** â€” unchanged |
| `assets/` | âœ… 5 offers (1 poisoned) | âœ… **Identical** â€” same offers |
| `guardrails.py` | âŒ Does not exist | ğŸ†• **NEW** â€” Prompt Shield scanner |
| `main.py` | âœ… Load â†’ Analyze â†’ Rank | âœ… **Modified** â€” Load â†’ **Scan** â†’ Analyze â†’ Rank |

**The entire security improvement is ~100 lines of code in two files.**

## What Changed

1. **`guardrails.py` (NEW)** â€” `PromptShieldScanner` class that sanitizes documents, separates visible from hidden content (e.g. HTML comments, hidden markup), and scans each segment independently through Azure AI Content Safety Prompt Shields API
2. **`main.py` (MODIFIED)** â€” Added a scanning step between loading offers and running the Flock agent; flagged offers are excluded before analysis
3. **Everything else â€” IDENTICAL** to Demo 1

## ğŸ§  Use Case

Same scenario as Demo 1: An AI agent analyzes vendor offer documents and produces an objective ranking based on price, timeline, risk, and scope.

The difference: **before** the offers reach the LLM, each document is sanitized and scanned by Azure AI Content Safety Prompt Shields. The scanner separates visible content from hidden segments (HTML comments, hidden markup, zero-width characters), then scans each part independently. The poisoned ACME offer â€” which contains hidden instructions designed to manipulate the ranking â€” is detected and blocked. Only clean offers proceed to the Flock agent for analysis.

## ğŸ¬ Running the Demo

### Prerequisites

- **Azure OpenAI access** â€” Deployment with GPT-4.1 model
- **Azure AI Content Safety** â€” Resource with Prompt Shields enabled
- **Docker** (recommended) or **Python 3.10+** with **UV package manager**

### Setup

```bash
# Navigate to demo directory
cd demos/demo4-prompt-shield

# Ensure .env has Azure OpenAI AND Content Safety credentials
cp ../../.env.example ../../.env
# Edit ../../.env and add:
#   AZURE_CONTENT_SAFETY_ENDPOINT=https://<your-resource>.cognitiveservices.azure.com
#   AZURE_CONTENT_SAFETY_KEY=<your-key>
```

### Execution

```bash
docker compose up --build
```

## How It Works

```
Step 1: Load Offers           â†’ Same as Demo 1 (loader.py reads offer-*.md files)
Step 2: Sanitize & Separate   â†’ NEW: Strip markup, split visible from hidden content
Step 3: Prompt Shield Scan    â†’ NEW: Each segment scanned via Azure AI Content Safety API
Step 4: Filter Results         â†’ NEW: Flagged offers removed from batch
Step 5: Flock Agent Analysis   â†’ Same as Demo 1 (agent.py ranks remaining offers)
Step 6: Display Results        â†’ Same as Demo 1 (ranking table output)
```

The Prompt Shield scanner first sanitizes each document â€” stripping HTML/XML tags, removing zero-width Unicode characters, and separating hidden content (e.g. HTML comments) from visible text. Each segment is then sent independently to the Azure AI Content Safety `text:shieldPrompt` endpoint. This prevents hidden injections from being diluted by surrounding document text.

```json
{
  "userPrompt": "Analyze and rank this vendor offer objectively.",
  "documents": [
    "<sanitized visible content>",
    "---BEGIN EXTERNAL CONTENT---\n<extracted hidden content>\n---END EXTERNAL CONTENT---"
  ]
}
```

The response includes `documentsAnalysis[].attackDetected` â€” if `true`, the document contains a detected prompt injection attempt.

**Fail-closed design:** If the API call fails for any reason (network error, timeout, auth failure), the document is treated as unsafe and excluded from analysis.

## ğŸ¯ Demo Flow

### Demo 1 (Vulnerable) â€” No scanning

```mermaid
sequenceDiagram
    participant User
    participant Agent as Flock Agent
    participant Offers as Offer Documents
    participant LLM

    User->>Agent: Analyze and rank offers
    Agent->>Offers: Load all offer files
    Offers-->>Agent: 5 offers (including poisoned ACME)

    Note over Offers: âš ï¸ ACME offer contains<br/>hidden injection instructions

    Agent->>LLM: All 5 offers sent to LLM
    LLM-->>Agent: Rankings with ACME at #1<br/>(manipulated by injection)
    Agent->>User: âŒ ACME wins with fake metrics
```

### Demo 4 (Secured) â€” Prompt Shield pre-screening

```mermaid
sequenceDiagram
    participant User
    participant Filter as Content Filter
    participant Shield as Prompt Shield
    participant Agent as Flock Agent
    participant Offers as Offer Documents
    participant LLM

    User->>Agent: Analyze and rank offers
    Agent->>Offers: Load all offer files
    Offers-->>Agent: 5 offers (including poisoned ACME)

    loop For each offer
        Agent->>Filter: Sanitize & separate content
        Filter-->>Filter: Strip markup, extract hidden segments
        Filter-->>Shield: Visible content + hidden segments
        Shield-->>Agent: âœ… / ğŸš¨ per segment
    end

    Note over Filter,Shield: ACME hidden injection detected!<br/>Poisoned offer blocked BEFORE reaching the LLM

    Agent->>LLM: Only 4 safe offers sent
    LLM-->>Agent: Objective ranking<br/>(no manipulation possible)
    Agent->>User: âœ… Correct ranking based on real metrics
```

## Azure AI Content Safety Prompt Shields

[Prompt Shields](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection) is an Azure AI Content Safety feature that detects prompt injection attacks in both user prompts and documents/grounding data.

**Key concepts:**

- **`userPrompt`** â€” The legitimate instruction given to the LLM (the agent's task)
- **`documents`** â€” External/untrusted content that will be processed by the LLM
- **`documentsAnalysis[].attackDetected`** â€” Whether the document contains instructions that attempt to override the user prompt

This maps directly to the indirect prompt injection pattern: the offer documents are untrusted external data that should contain only factual information, not LLM instructions.

## OWASP Mapping

| OWASP Reference | Description | Relevance |
|-----------------|-------------|-----------|
| **[LLM01: Prompt Injection](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)** | Manipulating LLM via crafted inputs | The poisoned ACME offer exploits this; Prompt Shield detects it |
| **[AG01: Agent Goal Hijacking](https://genai.owasp.org/resource/owasp-top-10-for-agentic-applications-for-2026/)** | Redirecting an agent from its intended task | The injection redirects the ranking agent; pre-screening prevents this |
| **[AG08: External Data & Configuration Poisoning](https://genai.owasp.org/resource/owasp-top-10-for-agentic-applications-for-2026/)** | Malicious data in external sources | Offer documents are external data; scanning catches poisoned content |

## ğŸ”‘ Key Takeaways

### Why Prompt Shield catches what traditional sanitization misses

- **Sanitize and separate** â€” Documents are preprocessed to strip markup and isolate hidden content (HTML comments, hidden elements, zero-width characters) before scanning
- **Semantic detection** â€” Prompt Shield uses ML models trained on injection patterns, not regex or keyword matching
- **Context-aware** â€” It understands the difference between legitimate content and instruction-like text embedded in documents
- **Evolving defense** â€” As attack techniques evolve, the API's detection models are updated by Microsoft

### Design principles demonstrated

- **Fail closed** â€” If the scanner can't reach the API, documents are treated as unsafe (never fail open)
- **Minimal code change** â€” The entire security layer is ~100 lines added to an existing application
- **Defense in depth** â€” Prompt Shield is one layer; it complements (not replaces) prompt engineering, output validation, and other controls
- **Pre-processing gate** â€” Scanning happens before LLM invocation, preventing poisoned content from ever entering the model's context

### What this demo proves

In Demo 1, the same agent with the same offers produces a **manipulated ranking** because the poisoned ACME offer hijacks the LLM's evaluation. In Demo 4, adding a single pre-processing step â€” Prompt Shield scanning â€” **detects and removes the poisoned offer**, resulting in an objective ranking based only on legitimate data. The agent code itself never needed to change.
